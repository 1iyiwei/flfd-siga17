<html>
<head>
<title>
Perceptually-Guided Foveation for Light Field Displays
</title>
<link rel="stylesheet" type="text/css" href="./styles/wei.css" />
</head>

<body  bgcolor="#FFFFFF">

<h1>
<center>
Perceptually-Guided Foveation for Light Field Displays
</center>
</h1>

<p align="center">
Qi Sun, Fu-Chung Huang, Joohwan Kim, Li-Yi Wei, David Luebke, and Arie Kaufman
</p>

<p align="center">
SIGGRAPH Asia 2017
</p>

<center>
<img src="./images/c121-f121_1-a56-representative_image-v3.png" width=80%>
</center>

<h2> Abstract </h2>

<blockquote>

A variety of applications such as virtual reality and immersive cinema require high image quality, low rendering latency, and consistent depth cues.
4D light field displays support focus accommodation, but are more costly to render than 2D images, resulting in higher latency.

The human visual system can resolve higher spatial frequencies in the fovea than in the periphery.
This property has been harnessed by recent 2D foveated rendering methods to reduce computation cost while maintaining perceptual quality.
Inspired by this, we present foveated 4D light fields by investigating their effects on 3D depth perception.
Based on our psychophysical experiments and theoretical analysis on visual and display bandwidths, we formulate a content-adaptive importance model in the 4D ray space.
We verify our method by building a prototype light field display that can render only 16%-30% rays without compromising perceptual quality.

</blockquote>
  
<h2> Available information </h2>

<blockquote>

<!--
[[paper](https://doi.org/10.1145/3130800.3130807)]
-->
[<a href="http://research.nvidia.com/sites/default/files/publications/c121-f121_199-a18-paperfinal-v3.pdf">paper</a>]
[<a href="https://arxiv.org/abs/1708.06034">perceptual studies</a>]
[<a href="https://youtu.be/tx0i30vTqMU">video (youtube)</a>]
[<a href="http://research.nvidia.com/publication/2017-11_Perceptually-Guided-Foveation-for">NVIDIA page</a>]

</blockquote>

<h2> Acknowledgement </h2>

<blockquote>

We would like to thank Ia-Ju Chiang and Suwen Zhu for helping us conducting the experiments; Anjul Patney, Kaan Ak≈üit, Piotr Didyk, Chris Wyman, and the anonymous reviewers for their valuable
suggestions. This work has been partially supported by National Science Foundation grants IIP1069147, CNS1302246, NRT1633299, CNS1650499, and Hong Kong RGC general research fund 17202415.

</blockquote>

</body>
</html>
